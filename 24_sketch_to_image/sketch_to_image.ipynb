{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sketch to Image\n",
        "1. Load data - just images - tensorflow dataset tfds google images\n",
        "1. create sketch for each image - this will be the input (X) and the original image will be the output (Y)\n",
        "1. Pass input sketch through an Encoder (resnet trained over imagenet - Frozen) to get a vector\n",
        "1. Pass the vector through Generator (trainable)\n",
        "1. Pass generated image through discriminator (Trainable)\n",
        "1. train GAN this way"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJpQtGG0HY4W"
      },
      "source": [
        "# import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YesWZLbHY4Z"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as Layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDmPOMv6HY4k"
      },
      "source": [
        "# define parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHmSx0zUHY4p"
      },
      "outputs": [],
      "source": [
        "LOSS = 'binary_crossentropy'\n",
        "OPTIMIZER_D = tf.keras.optimizers.RMSprop(lr=0.003, clipvalue=1.0, decay=6e-8)\n",
        "OPTIMIZER_A = tf.keras.optimizers.RMSprop(lr=0.002, clipvalue=1.0, decay=6e-8)\n",
        "METRICS = 'accuracy'\n",
        "\n",
        "INPUT_LENGTH = 128\n",
        "\n",
        "EPOCHS = 5# 1000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "INPUT_SHAPE = (28, 28, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN_2kcSGHY4w"
      },
      "source": [
        "# load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rO8WuyJAHY4z"
      },
      "outputs": [],
      "source": [
        "input_data = np.expand_dims(tf.keras.datasets.mnist.load_data()[0][0], axis=-1).astype('float')\n",
        "print(input_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def img_to_sketch(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "    invert = cv2.bitwise_not(gray)\n",
        "    blur = cv2.GaussianBlur(invert, (21, 21), 0)\n",
        "    invertedblur = cv2.bitwise_not(blur)\n",
        "    sketch = cv2.divide(gray, invertedblur, scale=256.0)\n",
        "    return sketch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kf1TK-f8HY49"
      },
      "source": [
        "# define model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encoder():\n",
        "    return tf.keras.applications.resnet50.ResNet50(include_top=False, weights='imagenet', input_shape=INPUT_SHAPE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGyDMvxfHY49"
      },
      "source": [
        "### generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OgyUghBHY4_"
      },
      "outputs": [],
      "source": [
        "def generator():\n",
        "    dim = INPUT_SHAPE[0]//4\n",
        "    depth = 256\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(Layers.Dense(dim*dim*depth, input_dim=INPUT_LENGTH))\n",
        "    model.add(Layers.LeakyReLU())\n",
        "    #model.add(Layers.BatchNormalization(momentum=0.9))\n",
        "    model.add(Layers.Reshape((dim, dim, depth)))\n",
        "    model.add(Layers.Conv2DTranspose(depth//4, 3, padding='same'))\n",
        "    model.add(Layers.LeakyReLU())\n",
        "    #model.add(Layers.BatchNormalization(momentum=0.9))\n",
        "    model.add(Layers.UpSampling2D(2))\n",
        "    model.add(Layers.Conv2DTranspose(depth//16, 3, padding='same'))\n",
        "    model.add(Layers.LeakyReLU())\n",
        "    #model.add(Layers.BatchNormalization(momentum=0.9))\n",
        "    model.add(Layers.UpSampling2D(2))\n",
        "    model.add(Layers.Conv2DTranspose(depth//64, 3, padding='same'))\n",
        "    model.add(Layers.LeakyReLU())\n",
        "    #model.add(Layers.BatchNormalization(momentum=0.9))\n",
        "    model.add(Layers.Conv2DTranspose(INPUT_SHAPE[2], 3, activation='tanh', padding='same'))\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9G5uUWTQHY5G"
      },
      "source": [
        "### discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lunHJFF_HY5J"
      },
      "outputs": [],
      "source": [
        "def discriminator():\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(Layers.Conv2D(32, 3, padding='same', input_shape=INPUT_SHAPE))\n",
        "    model.add(Layers.LeakyReLU())\n",
        "    model.add(Layers.Dropout(0.4))\n",
        "    model.add(Layers.Conv2D(64, 3, padding='same'))\n",
        "    model.add(Layers.LeakyReLU())\n",
        "    model.add(Layers.Dropout(0.4))\n",
        "    model.add(Layers.Conv2D(128, 3, padding='same'))\n",
        "    model.add(Layers.LeakyReLU())\n",
        "    model.add(Layers.Dropout(0.4))\n",
        "    model.add(Layers.MaxPooling2D(2))\n",
        "    model.add(Layers.Conv2D(64, 3, padding='same'))\n",
        "    model.add(Layers.LeakyReLU())\n",
        "    model.add(Layers.Flatten())\n",
        "    model.add(Layers.Dense(128))\n",
        "    model.add(Layers.LeakyReLU())\n",
        "    model.add(Layers.Dense(1, activation='sigmoid'))\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### generator model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gen_model(enc, gen):\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(enc(trainable=False))\n",
        "    model.add(gen)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIck5y-5HY5Q"
      },
      "source": [
        "### discriminator model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siWK-jHWHY5R"
      },
      "outputs": [],
      "source": [
        "def disc_model(discr):\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(discr)\n",
        "    model.compile(loss=LOSS, optimizer=OPTIMIZER_D, metrics=METRICS)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7UGEVBZHY5X"
      },
      "source": [
        "### adversarial model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxARx02xHY5Z"
      },
      "outputs": [],
      "source": [
        "def adversarial_model(gen, discr):\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(gen)\n",
        "    model.add(discr(trainable=False))\n",
        "    model.compile(loss=LOSS, optimizer=OPTIMIZER_A, metrics=METRICS)\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9K8gwm9HY5e"
      },
      "source": [
        "# Full model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwaOzIp_HY5f"
      },
      "outputs": [],
      "source": [
        "encdr = encoder()\n",
        "genrt = generator()\n",
        "discrim = discriminator()\n",
        "\n",
        "gen = gen_model(encdr, genrt)\n",
        "discr = disc_model(discrim)\n",
        "advr = adversarial_model(gen, discrim) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_input(batch_size):\n",
        "    return input_data[np.random.randint(0, input_data.shape[0], size=batch_size), :, :, :].map(lambda img : encdr(img_to_sketch(img)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9Yl8azYHY5l"
      },
      "outputs": [],
      "source": [
        "def get_noise(batch_size):\n",
        "    return np.random.uniform(-1.0, 1.0, size=[batch_size, INPUT_LENGTH])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOe8U_FQHY5r"
      },
      "outputs": [],
      "source": [
        "def get_pos_data(batch_size):\n",
        "    return input_data[np.random.randint(0, input_data.shape[0], size=batch_size), :, :, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8Q4a-IHHY5x"
      },
      "outputs": [],
      "source": [
        "def get_neg_data(batch_size):\n",
        "    images = gen.predict(get_input(batch_size))\n",
        "    return images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qk7dREDHY55"
      },
      "outputs": [],
      "source": [
        "def get_train_labels(batch_size):\n",
        "    labels = np.ones([2*batch_size, 1])\n",
        "    labels[batch_size:] = 0\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_kPnx18HY5_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrfZ9JcHHY6E"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42nXj1PEHY6F"
      },
      "outputs": [],
      "source": [
        "def train_discr():\n",
        "    data_pos = get_pos_data(BATCH_SIZE)\n",
        "    data_neg = get_neg_data(BATCH_SIZE)\n",
        "    data = np.append(data_pos, data_neg, axis=0)\n",
        "    labels = get_train_labels(BATCH_SIZE)\n",
        "    return discr.train_on_batch(data, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_G4nbqTHY6L"
      },
      "outputs": [],
      "source": [
        "def train_advr():\n",
        "    noise = get_input(BATCH_SIZE)\n",
        "    labels = np.ones([BATCH_SIZE, 1])\n",
        "    return advr.train_on_batch(noise, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGQUPhzDHY6P"
      },
      "outputs": [],
      "source": [
        "def train_GAN():\n",
        "    for i in range(EPOCHS):\n",
        "        loss_discr = train_discr()\n",
        "        loss_advr = train_advr()\n",
        "        log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, loss_discr[0], loss_discr[1])\n",
        "        log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, loss_advr[0], loss_advr[1])\n",
        "        print(log_mesg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9mvdiCOHY6U"
      },
      "outputs": [],
      "source": [
        "train_GAN()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWFwPdRLHY6Y"
      },
      "source": [
        "# Generate new images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyR9VFC5HY6a"
      },
      "outputs": [],
      "source": [
        "num_images = 10\n",
        "noise = get_input(num_images)\n",
        "generated_images = gen.predict(noise)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU3sb0OhHY6e"
      },
      "source": [
        "# View generated images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bL9M36XBHY6g"
      },
      "outputs": [],
      "source": [
        "plt.imshow(np.squeeze(generated_images[np.random.randint(num_images)]), cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctRvmrcSNas5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GAN.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
