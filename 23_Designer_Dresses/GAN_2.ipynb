{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJpQtGG0HY4W"
      },
      "source": [
        "# import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import sys\n",
        "\n",
        "if sys.platform.startswith('win32'):\n",
        "    # Windows specific procedures\n",
        "    print(\"Windows\")\n",
        "    import os\n",
        "    os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YesWZLbHY4Z"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "import keras\n",
        "# import keras.layers as Layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDmPOMv6HY4k"
      },
      "source": [
        "# define parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHmSx0zUHY4p"
      },
      "outputs": [],
      "source": [
        "LOSS = 'binary_crossentropy'\n",
        "OPTIMIZER_D = tf.keras.optimizers.RMSprop()\n",
        "OPTIMIZER_A = tf.keras.optimizers.RMSprop()\n",
        "METRICS = 'accuracy'\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "INPUT_LENGTH = 128\n",
        "BUFFER_SIZE = 60000\n",
        "\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "INPUT_SHAPE = (28, 28, 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN_2kcSGHY4w"
      },
      "source": [
        "# load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "rO8WuyJAHY4z",
        "outputId": "6713f9f7-7077-4108-e4cd-7a966c32d127"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "input_data = np.expand_dims(x_train, axis=-1).astype('float32')\n",
        "input_data = (input_data)/255.0\n",
        "\n",
        "print(input_data.shape)\n",
        "print(np.max(input_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DVEykB7yDb1"
      },
      "outputs": [],
      "source": [
        "# Batch and shuffle the data\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(input_data).shuffle(BUFFER_SIZE)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kf1TK-f8HY49"
      },
      "source": [
        "# define model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGyDMvxfHY49"
      },
      "source": [
        "### generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OgyUghBHY4_"
      },
      "outputs": [],
      "source": [
        "def generator():\n",
        "    dim = INPUT_SHAPE[0]//4\n",
        "    depth = 256\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    model.add(Layers.Dense(dim*dim*depth, use_bias=False, input_dim=INPUT_LENGTH))\n",
        "    model.add(Layers.BatchNormalization())\n",
        "    model.add(Layers.LeakyReLU())\n",
        "\n",
        "    model.add(Layers.Reshape((dim, dim, depth)))\n",
        "\n",
        "    model.add(Layers.Conv2DTranspose(depth//2, 7, strides=1, padding='same', use_bias=False))\n",
        "    model.add(Layers.BatchNormalization())\n",
        "    model.add(Layers.LeakyReLU())\n",
        "\n",
        "    model.add(Layers.Conv2DTranspose(depth//4, 5, strides=2, padding='same', use_bias=False))\n",
        "    model.add(Layers.BatchNormalization())\n",
        "    model.add(Layers.LeakyReLU())\n",
        "\n",
        "    model.add(Layers.Conv2DTranspose(INPUT_SHAPE[2], 3, strides=2, activation='tanh', padding='same', use_bias=False))\n",
        "\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9G5uUWTQHY5G"
      },
      "source": [
        "### discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lunHJFF_HY5J"
      },
      "outputs": [],
      "source": [
        "def discriminator():\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    model.add(Layers.Conv2D(64, 5, strides=2, padding='same', input_shape=INPUT_SHAPE))\n",
        "    model.add(Layers.LeakyReLU())\n",
        "    model.add(Layers.Dropout(0.3))\n",
        "\n",
        "    model.add(Layers.Conv2D(128, 5, strides=2, padding='same'))\n",
        "    model.add(Layers.LeakyReLU())\n",
        "\n",
        "    model.add(Layers.Flatten())\n",
        "\n",
        "    model.add(Layers.Dense(1, activation='sigmoid'))\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9K8gwm9HY5e"
      },
      "source": [
        "# Full model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 977
        },
        "id": "uwaOzIp_HY5f",
        "outputId": "c6213a78-a127-4768-c5d0-45686b1921a8"
      },
      "outputs": [],
      "source": [
        "gen = generator()\n",
        "discr = discriminator()\n",
        "\n",
        "discr.compile(loss=LOSS, optimizer=OPTIMIZER_D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZAr7ktnrvos"
      },
      "outputs": [],
      "source": [
        "advr = tf.keras.models.Sequential([gen, discr])\n",
        "advr.compile(loss=LOSS, optimizer=OPTIMIZER_A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R96Q4FDvtb3p"
      },
      "source": [
        "# Define train step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFTDA24fvHYs"
      },
      "outputs": [],
      "source": [
        "def train_step(images):\n",
        "    gen, discr = advr.layers\n",
        "    noise = tf.random.normal([BATCH_SIZE, INPUT_LENGTH])\n",
        "    fake_images = gen(noise)\n",
        "    x_d = tf.concat([images, fake_images], axis=0)\n",
        "    y_d = tf.constant([[1.0]]*BATCH_SIZE+[[0.0]]*BATCH_SIZE)\n",
        "    discr.trainable = True\n",
        "    discr.train_on_batch(x_d, y_d)\n",
        "    discr.trainable = False\n",
        "    y_a = tf.constant([[1.0]]*BATCH_SIZE)\n",
        "    advr.train_on_batch(noise, y_a)\n",
        "    #return gen_loss, disc_loss\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrfZ9JcHHY6E"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7wulqNzCNJE"
      },
      "outputs": [],
      "source": [
        "seed = tf.random.normal([16, INPUT_LENGTH])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_od1swuCDAC"
      },
      "outputs": [],
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9U1FRzOxzq3"
      },
      "outputs": [],
      "source": [
        "def train(dataset, epochs):\n",
        "    for e in range(epochs):\n",
        "      start = time.time()\n",
        "      for image_batch in dataset:\n",
        "          train_step(image_batch)\n",
        "      # Produce images for the GIF as we go\n",
        "      display.clear_output(wait=True)\n",
        "      generate_and_save_images(gen, e + 1, seed)\n",
        "\n",
        "      print ('Time for epoch {} is {} sec'.format(e + 1, time.time()-start))\n",
        "\n",
        "    # Generate after the final epoch\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_images(gen, epochs, seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "i9mvdiCOHY6U",
        "outputId": "1898c041-74ee-438d-9992-41daf3e97f11"
      },
      "outputs": [],
      "source": [
        "train(train_dataset, EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWFwPdRLHY6Y"
      },
      "source": [
        "# Generate new images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyR9VFC5HY6a"
      },
      "outputs": [],
      "source": [
        "num_images = 10\n",
        "noise = tf.random.normal([num_images, INPUT_LENGTH])\n",
        "generated_images = gen(noise, training=False)*255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU3sb0OhHY6e"
      },
      "source": [
        "# View generated images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bL9M36XBHY6g"
      },
      "outputs": [],
      "source": [
        "plt.imshow(np.squeeze(generated_images[np.random.randint(num_images)]), cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctRvmrcSNas5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "GAN_2.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "4df7b8dbe6363cad0757d94bedbaad70e41bc83c856a77273c9f997594d38424"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
